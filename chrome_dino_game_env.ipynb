{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "055ffa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.13\r\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete, Dict\n",
    "from mss import mss\n",
    "import pytesseract\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import pygame as pg\n",
    "import random\n",
    "from gym.envs.registration import register\n",
    "from setuptools import setup\n",
    "!{sys.executable} --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccf0572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sprite():\n",
    "    def __init__(self, img, pos, game):\n",
    "        self.position = np.array(pos, dtype=float)\n",
    "        self.image = img\n",
    "        self.game = game\n",
    "        if type(self.image) == list:\n",
    "            self.height = self.image[0].get_rect()[3]\n",
    "            self.width = self.image[0].get_rect()[2]\n",
    "        else:\n",
    "            self.height = self.image.get_rect()[3]\n",
    "            self.width = self.image.get_rect()[2]\n",
    "        self.game.all_sprites.append(self)\n",
    "        \n",
    "    def update(self):\n",
    "        self.position[0] -= self.game.game_speed\n",
    "        if self.position[0] < -150:\n",
    "            self.reset()\n",
    "        self.height = self.image.get_rect()[3]\n",
    "        self.width = self.image.get_rect()[2]\n",
    "        \n",
    "    def get_obs(self):\n",
    "        return np.append([self.position], [self.width, self.height])\n",
    "        \n",
    "    def draw(self, screen):\n",
    "        screen.blit(self.image, self.position)\n",
    "        #pg.draw.rect(screen, (255,0,0), (self.position, (self.width, self.height)), 2)\n",
    "\n",
    "    def reset(self):\n",
    "        # we need to find a valid location for the reset position in order to not make the game impossible\n",
    "        min_dist = self.game.game_speed * 20\n",
    "        max_dist = self.game.game_speed * 60\n",
    "        max_x_pos = 0\n",
    "        for obstacle in self.game.obstacles:\n",
    "            if obstacle.position[0] > max_x_pos:\n",
    "                max_x_pos = obstacle.position[0]\n",
    "        self.position[0] = max(max_x_pos + random.randint(min_dist, max_dist), 1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d282a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Background(Sprite):\n",
    "    def __init__(self, img, pos, game, secondary=True):\n",
    "        super(Background, self).__init__(img, pos, game)\n",
    "        self.secondary = secondary\n",
    "        if self.secondary:\n",
    "            new_bg = Background(self.image, (self.width, self.game.ground_height-15), self.game, secondary=False)\n",
    "    \n",
    "    def update(self):\n",
    "        self.position[0] -= self.game.game_speed\n",
    "        if self.position[0] <= -self.width+20:\n",
    "            self.small_reset()\n",
    "        self.height = self.image.get_rect()[3]\n",
    "        self.width = self.image.get_rect()[2]\n",
    "        \n",
    "    def small_reset(self):\n",
    "        self.position[0] = self.width\n",
    "    \n",
    "    def reset(self):\n",
    "        self.position[0] = 0\n",
    "        if self.secondary:\n",
    "            self.position[0] = self.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c876b2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cloud(Sprite):\n",
    "    def __init__(self, img, pos, game):\n",
    "        super(Cloud, self).__init__(img, pos, game)\n",
    "        self.width = self.image.get_width()\n",
    "            \n",
    "    def reset(self):\n",
    "        if self.position[0] < -self.width:\n",
    "            self.position[0] = 1100 + random.randint(2500,3000)\n",
    "            self.position[1] = random.randint(50,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ffeba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cactus(Sprite):\n",
    "    def __init__(self, img, pos, game):\n",
    "        super(Cactus, self).__init__(img, pos, game)\n",
    "        self.position[1] = self.game.ground_height - self.height\n",
    "        self.game.obstacles.append(self)\n",
    "        self.imgs = self.image\n",
    "        self.image = random.choice(self.imgs)\n",
    "        \n",
    "    def update(self):\n",
    "        super(Cactus, self).update()\n",
    "        self.position[1] = self.game.ground_height - self.height\n",
    "        \n",
    "    def reset(self):\n",
    "        super(Cactus, self).reset()\n",
    "        self.image = random.choice(self.imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "718b74cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bird(Sprite):\n",
    "    def __init__(self, img, pos, game):\n",
    "        super(Bird, self).__init__(img, pos, game)\n",
    "        self.game.obstacles.append(self)\n",
    "        self.imgs = self.image\n",
    "        self.image = self.imgs[0]\n",
    "        self.step_index = 0\n",
    "        self.elevation = random.randint(100,250)\n",
    "        \n",
    "    def update(self):\n",
    "        self.position[1] = self.game.ground_height - self.elevation\n",
    "        super(Bird, self).update()\n",
    "        if self.step_index >= 9:\n",
    "            self.step_index = 0\n",
    "        self.image = self.imgs[self.step_index//5]\n",
    "        self.step_index += 1\n",
    "        \n",
    "    def reset(self):\n",
    "        super(Bird, self).reset()\n",
    "        self.elevation = random.randint(50,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac8b53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dino(Sprite):\n",
    "    def __init__(self, img, pos, game):\n",
    "        super(Dino, self).__init__(img, pos, game)\n",
    "        self.imgs = img\n",
    "        self.duck_offset = np.array([0,33], dtype=float)\n",
    "        self.jump_vel = 8.5\n",
    "        \n",
    "        # running, jumping, ducking\n",
    "        self.state = np.array([True, False, False], dtype=bool)\n",
    "        self.new_state = self.state\n",
    "        self.step_index = 0\n",
    "        \n",
    "        self.position[1] = self.game.ground_height - self.height\n",
    "        self.image = self.imgs[0]\n",
    "        \n",
    "    def update(self):\n",
    "        pass\n",
    "        \n",
    "    def reset(self):\n",
    "        self.position = np.array([10,self.game.ground_height-self.height], dtype=float)\n",
    "        self.image = self.imgs[0]\n",
    "                \n",
    "    def act(self, user_input):\n",
    "        if self.step_index >= 10:\n",
    "            self.step_index = 0\n",
    "            \n",
    "        actions = [self.run, self.jump, self.duck]\n",
    "        self.new_state = np.zeros(3, dtype=bool)\n",
    "        \n",
    "        # jump if not already jumping\n",
    "        if user_input[pg.K_UP]:\n",
    "            self.new_state = np.array([False, True, False])\n",
    "        # duck if not already jumping\n",
    "        elif user_input[pg.K_DOWN]:            \n",
    "            self.new_state = np.array([False, False, True])\n",
    "        # if not still jumping, run (do nothing)\n",
    "        else:\n",
    "            self.new_state = np.array([True, False, False])\n",
    "            \n",
    "        # unless the player is still in the air (jumping), apply new action\n",
    "        if not self.state[1]:\n",
    "            for state, t_value in enumerate(self.new_state):\n",
    "                if t_value: actions[state]()\n",
    "            self.state = self.new_state\n",
    "        else:\n",
    "            actions[1]()\n",
    "            \n",
    "    def take_action(self, choice):\n",
    "        if self.step_index >= 10:\n",
    "            self.step_index = 0\n",
    "            \n",
    "        actions = [self.run, self.jump, self.duck]\n",
    "        self.new_state = np.zeros(3, dtype=bool)\n",
    "        self.new_state[choice] = True\n",
    "            \n",
    "        # unless the player is still in the air (jumping), apply new action\n",
    "        if not self.state[1]:\n",
    "            actions[choice]()\n",
    "            self.state = self.new_state\n",
    "        else:\n",
    "            actions[1]()\n",
    "                \n",
    "    def run(self):\n",
    "        # if not currently jumping or about to duck, run (do nothing)\n",
    "        if not self.state[1] or self.new_state[2] or self.state[2]:\n",
    "            self.image = self.imgs[:2][self.step_index // 5]\n",
    "            self.step_index += 1\n",
    "            self.position[1] = self.game.ground_height - self.height\n",
    "    \n",
    "    def jump(self):\n",
    "        # jump\n",
    "        self.image = self.imgs[2]\n",
    "        if self.state[1]:\n",
    "            self.position[1] -= self.jump_vel * 4\n",
    "            self.jump_vel -= 0.8\n",
    "        if self.position[1] + self.height >= self.game.ground_height:\n",
    "            self.state[1] = False\n",
    "            self.jump_vel = 10\n",
    "            self.position[1] = self.game.ground_height - self.height\n",
    "    \n",
    "    def duck(self):\n",
    "        if not self.state[1]:\n",
    "            self.image = self.imgs[3:5][self.step_index // 5]\n",
    "            self.step_index += 1\n",
    "        # unless the player was already ducking, we apply an offset to the position\n",
    "        if not self.state[2]:\n",
    "            self.position[1] = self.game.ground_height - self.height\n",
    "            self.position += self.duck_offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b919d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    def __init__(self):\n",
    "        #pg.init()\n",
    "        #global ground_height, game_speed, all_sprites, obstacles\n",
    "       # ground_height = 380\n",
    "       # game_speed = 14\n",
    "       # all_sprites = []\n",
    "       # obstacles = []\n",
    "        self.points = 0\n",
    "        \n",
    "        self.ground_height = 380\n",
    "        self.game_speed = 14\n",
    "        self.all_sprites = []\n",
    "        self.obstacles = []  \n",
    "        \n",
    "        imgs = []\n",
    "        assets = ['Assets/'+i for i in ['Cactus', 'Bird', 'Dino', 'Other']]\n",
    "        all_assets = [['SmallCactus1.png', 'SmallCactus2.png', 'SmallCactus3.png','LargeCactus1.png', 'LargeCactus2.png', 'LargeCactus3.png'], ['Bird1.png', 'Bird2.png'], ['DinoRun1.png', 'DinoRun2.png', 'DinoJump.png', 'DinoDuck1.png', 'DinoDuck2.png'], ['Cloud.png', 'GameOver.png', 'Reset.png', 'Track.png']]\n",
    "        for idx, asset in enumerate(assets):\n",
    "            imgs.append([pg.image.load(os.path.join(asset, i)) for i in all_assets[idx]])\n",
    "            \n",
    "        cactus_imgs, bird_imgs, dino_imgs, other_imgs = imgs\n",
    "        imgs = [item for sublist in imgs for item in sublist]\n",
    "        all_assets = [item for sublist in all_assets for item in sublist]\n",
    "        for idx, img in enumerate(imgs):\n",
    "            print(f'{all_assets[idx]}: {img.get_rect()}')\n",
    "        \n",
    "        self.player = Dino(dino_imgs, (10,333), self)\n",
    "        self.cactus= Cactus(cactus_imgs, (1100+random.randint(200,700), 300), self)\n",
    "        #self.bird = Bird(bird_imgs, (1100+random.randint(800,1300), 250), self)\n",
    "        self.background = Background(other_imgs[3], (0,self.ground_height-15), self)\n",
    "        self.cloud = Cloud(other_imgs[0], (1100+random.randint(800,1000),(random.randint(50,100))), self)\n",
    "        \n",
    "        #for sprite in self.all_sprites:\n",
    "         #   print(sprite.get_obs())\n",
    "            \n",
    "    def reset(self):\n",
    "        self.game_speed = 14\n",
    "        self.points = 0\n",
    "        for sprite in self.all_sprites:\n",
    "            sprite.reset()\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        pg.init()\n",
    "        running = True\n",
    "        clock = pg.time.Clock()\n",
    "        self.screen = pg.display.set_mode((1100, 600))\n",
    "        \n",
    "        while running:\n",
    "            for event in pg.event.get():\n",
    "                if event.type == pg.QUIT:\n",
    "                    running = False\n",
    "            self.screen.fill((255,255,255))\n",
    "            user_input = pg.key.get_pressed()\n",
    "            \n",
    "            self.score()\n",
    "            \n",
    "            self.player.act(user_input)\n",
    "            \n",
    "            for sprite in self.all_sprites:\n",
    "                sprite.draw(self.screen)\n",
    "                sprite.update()\n",
    "                \n",
    "                \n",
    "            game_over = False\n",
    "            for obstacle in self.obstacles:\n",
    "                if self.check_for_collision(obstacle):\n",
    "                    game_over = True\n",
    "            if game_over:\n",
    "                self.reset()\n",
    "            clock.tick(30)\n",
    "            pg.display.update()\n",
    "        \n",
    "        pg.display.quit()\n",
    "        pg.quit()\n",
    "        exit()\n",
    "        \n",
    "    def check_for_collision(self, obstacle):\n",
    "        # checks if two rectangles collide\n",
    "        o_x, o_y = obstacle.position\n",
    "        p_x, p_y = self.player.position\n",
    "        o_w, o_h = obstacle.width, obstacle.height\n",
    "        p_w, p_h = self.player.width, self.player.height\n",
    "        return o_x + o_w >= p_x and o_x <= p_x + p_w and o_y + o_h >= p_y and o_y <= p_y + p_h\n",
    "   \n",
    "        \n",
    "    def score(self):\n",
    "        global game_speed\n",
    "        self.points += 1\n",
    "        if self.points % 100 == 0:\n",
    "            self.game_speed +=1\n",
    "        font = pg.font.Font('freesansbold.ttf', 20)    \n",
    "        text = font.render('Score: ' + str(self.points), True, (0,0,0))\n",
    "        text_rect = text.get_rect()\n",
    "        text_rect.center = (1000,40)\n",
    "        self.screen.blit(text, text_rect)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96d33bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallCactus1.png: <rect(0, 0, 40, 71)>\n",
      "SmallCactus2.png: <rect(0, 0, 68, 71)>\n",
      "SmallCactus3.png: <rect(0, 0, 105, 71)>\n",
      "LargeCactus1.png: <rect(0, 0, 48, 95)>\n",
      "LargeCactus2.png: <rect(0, 0, 99, 95)>\n",
      "LargeCactus3.png: <rect(0, 0, 102, 95)>\n",
      "Bird1.png: <rect(0, 0, 97, 68)>\n",
      "Bird2.png: <rect(0, 0, 93, 62)>\n",
      "DinoRun1.png: <rect(0, 0, 87, 94)>\n",
      "DinoRun2.png: <rect(0, 0, 88, 94)>\n",
      "DinoJump.png: <rect(0, 0, 88, 94)>\n",
      "DinoDuck1.png: <rect(0, 0, 118, 60)>\n",
      "DinoDuck2.png: <rect(0, 0, 116, 60)>\n",
      "Cloud.png: <rect(0, 0, 84, 101)>\n",
      "GameOver.png: <rect(0, 0, 386, 40)>\n",
      "Reset.png: <rect(0, 0, 75, 101)>\n",
      "Track.png: <rect(0, 0, 2404, 28)>\n"
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "#game.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e24b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromeDinoEnv(Env):\n",
    "    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps':30}\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.window_size = (1100, 600)\n",
    "        \n",
    "        global ground_height, game_speed, all_sprites, obstacles\n",
    "        ground_height = 380\n",
    "        game_speed = 14\n",
    "        all_sprites = []\n",
    "        obstacles = []\n",
    "        self.points = 0\n",
    "        self.ground_height = 380\n",
    "        self.game_speed = 14\n",
    "        self.all_sprites = []\n",
    "        self.obstacles = []\n",
    "        \n",
    "        imgs = []\n",
    "        assets = ['Assets/'+i for i in ['Cactus', 'Bird', 'Dino', 'Other']]\n",
    "        all_assets = [['SmallCactus1.png', 'SmallCactus2.png', 'SmallCactus3.png','LargeCactus1.png', 'LargeCactus2.png', 'LargeCactus3.png'], ['Bird1.png', 'Bird2.png'], ['DinoRun1.png', 'DinoRun2.png', 'DinoJump.png', 'DinoDuck1.png', 'DinoDuck2.png'], ['Cloud.png', 'GameOver.png', 'Reset.png', 'Track.png']]\n",
    "        for idx, asset in enumerate(assets):\n",
    "            imgs.append([pg.image.load(os.path.join(asset, i)) for i in all_assets[idx]])\n",
    "            \n",
    "        cactus_imgs, bird_imgs, dino_imgs, other_imgs = imgs\n",
    "        \n",
    "        self.player = Dino(dino_imgs, (10,333), self)\n",
    "        self.cactus= Cactus(cactus_imgs, (1100+random.randint(200,700), 300), self)\n",
    "        #self.bird = Bird(bird_imgs, (1100+random.randint(800,1300), 250), self)\n",
    "        self.background = Background(other_imgs[3], (0,self.ground_height-15), self)\n",
    "        self.cloud = Cloud(other_imgs[0], (1100+random.randint(800,1000),(random.randint(50,100))), self)\n",
    "        \n",
    "        \n",
    "        # observations are dictionaries with the sprites rectangles (x, y, w, h)\n",
    "  #      self.observation_space = Dict(\n",
    "   #         {\n",
    "    #        \"player\": Box(0, 2000, shape=(4,), dtype=float),\n",
    "     #       \"bird\": Box(0, 2000, shape=(4,), dtype=float),\n",
    "      #      \"cactus\": Box(0, 2000, shape=(4,), dtype=float),\n",
    "       #     \"speed\": Box(0, 50000, shape=(1,), dtype=int)\n",
    "        #    }\n",
    "        #)\n",
    "        self.observation_space = Box(0, 5000, shape=(4,), dtype=float)\n",
    "       # self.observation_space = Box(low=np.array([0, -100, 0, 0]), high=np.array([300, 5000, 300, 10000]), dtype=float)\n",
    "        \n",
    "        # we have 3 actions: run(do nothing), jump, duck\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "        self.game_over = False\n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata['render_modes']\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        \n",
    "    def check_for_collision(self, obstacle):\n",
    "        # checks if two rectangles collide\n",
    "        o_x, o_y = obstacle.position\n",
    "        p_x, p_y = self.player.position\n",
    "        o_w, o_h = obstacle.width, obstacle.height\n",
    "        p_w, p_h = self.player.width, self.player.height\n",
    "        return o_x + o_w >= p_x and o_x <= p_x + p_w and o_y + o_h >= p_y and o_y <= p_y + p_h\n",
    "   \n",
    "        \n",
    "    def _get_obs(self):\n",
    "        # get location and size of relevant objects\n",
    "        #return {\"player\": self.player.get_obs(), \"bird\": self.bird.get_obs(), \"cactus\": self.cactus.get_obs(), \"speed\": game_speed}\n",
    "        p_obs = self.player.get_obs()\n",
    "        closest_obst = 5000\n",
    "        closest_height = 0\n",
    "        for obstacle in self.obstacles:\n",
    "            obst = obstacle.get_obs()\n",
    "            if obst[0] < closest_obst:\n",
    "                if obst[0] > 0:\n",
    "                    closest_obst = obst[0]\n",
    "                    closest_height = obst[1]\n",
    "        dist_to_obst = closest_obst - p_obs[0]\n",
    "        player_height = p_obs[1]\n",
    " #       dist_to_bird = self.bird.get_obs()[0] - (p_obs[0] + p_obs[2]) / 10\n",
    "  #      dist_to_cactus = self.cactus.get_obs()[0] - (p_obs[0] + p_obs[2])/10\n",
    "   #     bird_height = self.bird.get_obs()[1]\n",
    "    #    cactus_height = self.cactus.get_obs()[3]\n",
    "     #   cactus_width = self.cactus.get_obs()[2]\n",
    "        game_speed = self.game_speed\n",
    "        #return np.array([player_height, dist_to_bird, dist_to_cactus, bird_height, cactus_height, cactus_width, game_speed])\n",
    "        #return np.append(np.concatenate([self.player.get_obs(), self.bird.get_obs(), self.cactus.get_obs()]), game_speed)\n",
    "        return np.array([player_height, dist_to_obst, closest_height, game_speed])\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        # actions: jump, duck, do nothing\n",
    "        self.player.take_action(action)\n",
    "        for sprite in self.all_sprites:\n",
    "            sprite.update()\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        done = self.get_done()\n",
    "        reward = 1\n",
    "        \n",
    "  #      if observation[0] < 286:\n",
    "  #          reward = -0.5\n",
    "   #     \n",
    "    #    if observation[1] < 0:\n",
    "     #       reward = 3\n",
    "  #      if observation[4] <= 0 or observation [8] <= 0:\n",
    "   #         reward = 3\n",
    "\n",
    "            \n",
    "        info = self._get_info()\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "            \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == 'rgb_array':\n",
    "            return self._render_frame()\n",
    "        \n",
    "    def _render_frame(self):\n",
    "        global game_speed\n",
    "        if self.window is None and self.render_mode == 'human':\n",
    "            pg.init()\n",
    "            pg.font.init()\n",
    "            pg.display.init()\n",
    "            self.window = pg.display.set_mode((1100, 600))\n",
    "        if self.clock is None and self.render_mode == 'human':\n",
    "            self.clock = pg.time.Clock()\n",
    "        \n",
    "        self.screen = pg.Surface((1100, 600))\n",
    "        self.screen.fill((255,255,255))\n",
    "            \n",
    "        for sprite in self.all_sprites:\n",
    "            sprite.draw(self.screen)\n",
    "            \n",
    "        self.points += 1\n",
    "        if self.points % 100 == 0:\n",
    "            self.game_speed +=1\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            font = pg.font.Font('freesansbold.ttf', 20)    \n",
    "            text = font.render('Score: ' + str(self.points), True, (0,0,0))\n",
    "            text_rect = text.get_rect()\n",
    "            text_rect.center = (1000,40)\n",
    "            self.window.blit(self.screen, self.screen.get_rect())\n",
    "            self.window.blit(text, (1000,40))\n",
    "            pg.event.pump()\n",
    "            pg.display.update()\n",
    "            self.clock.tick(self.metadata['render_fps'])\n",
    "        else:\n",
    "            return np.transpose(\n",
    "            np.array(pg.surfarray.pixels3d(self.screen)), axes=(1,0,2))\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        # reset all sprites to a semi-random (specific to their class) location \n",
    "        for sprite in self.all_sprites:\n",
    "            sprite.reset()\n",
    "        self.points = 0\n",
    "        self.game_speed = 14\n",
    "        observation =  self._get_obs()\n",
    "        info = self._get_info()\n",
    "        return observation #, info\n",
    "    \n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pg.display.quit()\n",
    "            pg.quit()\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    " \n",
    "    def get_done(self):\n",
    "        done = False\n",
    "        for obstacle in self.obstacles:\n",
    "            if self.check_for_collision(obstacle):\n",
    "                done = True\n",
    "        return done\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return{'score': self.points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd66b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "    \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d50ce92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu')\n",
    "        self.fc3 = Dense(n_actions, activation='softmax')\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3952d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self, fc1_dims=256, fc2_dims=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu')\n",
    "        self.fc3 = Dense(1, activation=None)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97ba1647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95, policy_clip=0.2, batch_size=64, n_epochs=10, chkpt_dir='models/'):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        \n",
    "        self.actor = ActorNetwork(n_actions)\n",
    "        self.critic = CriticNetwork()\n",
    "        self.actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        \n",
    "        self.memory = PPOMemory(batch_size)\n",
    "        \n",
    "    def store_transition(self, state, actions, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "        \n",
    "    def save_models(self):\n",
    "        self.actor.save(self.chkpt_dir + 'actor')\n",
    "        self.critic.save(self.chkpt_dir + 'critic')\n",
    "        \n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor = keras.models.load_model(self.chkpt_dir + 'actor')\n",
    "        self.critic = keras.models.load_model(self.chkpt_dir + 'critic')\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        \n",
    "        probs = self.actor(state)\n",
    "        dist = tfp.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        action = action.numpy()[0]\n",
    "        value = value.numpy()[0]\n",
    "        log_prob = log_prob.numpy()[0]\n",
    "        \n",
    "        return action, log_prob, value\n",
    "    \n",
    "    def learn(self):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, dones_arr, batches = self.memory.generate_batches()\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "            \n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*(1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "                    \n",
    "            for batch in batches:\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    states = tf.convert_to_tensor(state_arr[batch])\n",
    "                    old_probs = tf.convert_to_tensor(old_prob_arr[batch])\n",
    "                    actions = tf.convert_to_tensor(action_arr[batch])\n",
    "                    \n",
    "                    probs = self.actor(states)\n",
    "                    dist = tfp.distributions.Categorical(probs)\n",
    "                    new_probs = dist.log_prob(actions)\n",
    "                    \n",
    "                    critic_value = self.critic(states)\n",
    "                    critic_value = tf.squeeze(critic_value, 1)\n",
    "                    \n",
    "                    prob_ratio = tf.math.exp(new_probs - old_probs)\n",
    "                    \n",
    "                    weighted_probs = advantage[batch] * prob_ratio\n",
    "                    clipped_probs = tf.clip_by_value(prob_ratio, 1-self.policy_clip, 1+self.policy_clip)\n",
    "                    weighted_clipped_probs = clipped_probs * advantage[batch]\n",
    "                    \n",
    "                    actor_loss = -tf.math.minimum(weighted_probs, weighted_clipped_probs)\n",
    "                    actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "                    \n",
    "                    returns = advantage[batch] + values[batch]\n",
    "                    critic_loss = keras.losses.MSE(critic_value, returns)\n",
    "                \n",
    "                actor_params = self.actor.trainable_variables\n",
    "                critic_params = self.critic.trainable_variables\n",
    "                actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "                critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "                self.actor.optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "                self.critic.optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "                \n",
    "        self.memory.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345c5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7722ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1840d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20690dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 286., 2419.,  309.,   14.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ChromeDinoEnv()\n",
    "env._get_obs()\n",
    "#env.observation_space.sample()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70476c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3 import DQN\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "829d8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a740d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, f'best_model{self.n_calls}')\n",
    "            self.model.save(model_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe5e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f97067f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)\n",
    "callback._init_callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94f2520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env, buffer_size=120000, learning_starts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3f1792f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cell is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5n/lggt_xf57s78jjmq538wdr6w0000gn/T/ipykernel_2325/187960133.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#model.save('dqn_chrome_dino')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         )\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mlearning_starts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_starts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                 \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             )\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;31m# Only stop training if return value is False, not when it is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mRolloutReturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_collected_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_collected_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5n/lggt_xf57s78jjmq538wdr6w0000gn/T/ipykernel_2325/2945986778.py\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_calls\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'best_model{self.n_calls}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, exclude, include)\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0mparams_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0msave_to_zip_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_to_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpytorch_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpytorch_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/stable_baselines3/common/save_util.py\u001b[0m in \u001b[0;36msave_to_zip_file\u001b[0;34m(save_path, data, params, pytorch_variables, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;31m# try to serialize them blindly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mserialized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;31m# Create a zip-archive and write our objects there.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/stable_baselines3/common/save_util.py\u001b[0m in \u001b[0;36mdata_to_json\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# from other languages/humans, so we have an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# idea what was being stored.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mbase64_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_item\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Use \":\" to make sure we do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"recursion\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m                 return self._save_reduce_pickle5(\n\u001b[0;32m--> 784\u001b[0;31m                     \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m                 )\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_save_reduce_pickle5\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    719\u001b[0m             self.save_reduce(\n\u001b[1;32m    720\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlistitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlistitems\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m                 \u001b[0mdictitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m             )\n\u001b[1;32m    723\u001b[0m             \u001b[0;31m# backport of the Python 3.8 state_setter pickle operations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m             \u001b[0;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/dill/_dill.py\u001b[0m in \u001b[0;36msave_cell\u001b[0;34m(pickler, obj)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ce: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_create_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# Ce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cell is empty"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=5000, callback=callback)\n",
    "#model.save('dqn_chrome_dino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0667845f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/actor/assets\n",
      "INFO:tensorflow:Assets written to: models/critic/assets\n",
      "episode 0 score -88.5 avg score -88.5 time_steps 571 learning_steps 28\n",
      "INFO:tensorflow:Assets written to: models/actor/assets\n",
      "INFO:tensorflow:Assets written to: models/critic/assets\n",
      "episode 1 score -3.0 avg score -45.8 time_steps 643 learning_steps 32\n",
      "INFO:tensorflow:Assets written to: models/actor/assets\n",
      "INFO:tensorflow:Assets written to: models/critic/assets\n",
      "episode 2 score -41.0 avg score -44.2 time_steps 895 learning_steps 44\n",
      "INFO:tensorflow:Assets written to: models/actor/assets\n",
      "INFO:tensorflow:Assets written to: models/critic/assets\n",
      "episode 3 score -6.0 avg score -34.6 time_steps 967 learning_steps 48\n",
      "INFO:tensorflow:Assets written to: models/actor/assets\n",
      "INFO:tensorflow:Assets written to: models/critic/assets\n",
      "episode 4 score -29.5 avg score -33.6 time_steps 1044 learning_steps 52\n",
      "episode 5 score -42.5 avg score -35.1 time_steps 1218 learning_steps 60\n",
      "INFO:tensorflow:Assets written to: models/actor/assets\n",
      "INFO:tensorflow:Assets written to: models/critic/assets\n",
      "episode 6 score -24.5 avg score -33.6 time_steps 1301 learning_steps 65\n",
      "INFO:tensorflow:Assets written to: models/actor/assets\n",
      "INFO:tensorflow:Assets written to: models/critic/assets\n",
      "episode 7 score -3.0 avg score -29.8 time_steps 1373 learning_steps 68\n",
      "episode 8 score -61.5 avg score -33.3 time_steps 1637 learning_steps 81\n",
      "episode 9 score -60.0 avg score -36.0 time_steps 1979 learning_steps 98\n",
      "episode 10 score -79.0 avg score -39.9 time_steps 2411 learning_steps 120\n",
      "episode 11 score -28.0 avg score -38.9 time_steps 2492 learning_steps 124\n",
      "episode 12 score -21.0 avg score -37.5 time_steps 2655 learning_steps 132\n",
      "episode 13 score -41.0 avg score -37.8 time_steps 2907 learning_steps 145\n",
      "episode 14 score -22.0 avg score -36.7 time_steps 3069 learning_steps 153\n",
      "episode 15 score -60.0 avg score -38.2 time_steps 3411 learning_steps 170\n",
      "episode 16 score -24.5 avg score -37.4 time_steps 3494 learning_steps 174\n",
      "episode 17 score -79.0 avg score -39.7 time_steps 3926 learning_steps 196\n",
      "episode 18 score -44.5 avg score -39.9 time_steps 4098 learning_steps 204\n",
      "episode 19 score -1.5 avg score -38.0 time_steps 4170 learning_steps 208\n",
      "episode 20 score -67.0 avg score -39.4 time_steps 4430 learning_steps 221\n",
      "episode 21 score -1.0 avg score -37.6 time_steps 4504 learning_steps 225\n",
      "episode 22 score -41.0 avg score -37.8 time_steps 4756 learning_steps 237\n",
      "episode 23 score -3.0 avg score -36.3 time_steps 4828 learning_steps 241\n",
      "episode 24 score -79.0 avg score -38.0 time_steps 5260 learning_steps 263\n",
      "episode 25 score -28.5 avg score -37.7 time_steps 5339 learning_steps 266\n",
      "episode 26 score -3.0 avg score -36.4 time_steps 5411 learning_steps 270\n",
      "episode 27 score -60.0 avg score -37.2 time_steps 5753 learning_steps 287\n",
      "episode 28 score -21.0 avg score -36.7 time_steps 5916 learning_steps 295\n",
      "episode 29 score -47.0 avg score -37.0 time_steps 6087 learning_steps 304\n",
      "episode 30 score -64.5 avg score -37.9 time_steps 6348 learning_steps 317\n",
      "episode 31 score -3.0 avg score -36.8 time_steps 6420 learning_steps 321\n",
      "episode 32 score -42.5 avg score -37.0 time_steps 6672 learning_steps 333\n",
      "episode 33 score 0.0 avg score -35.9 time_steps 6744 learning_steps 337\n",
      "episode 34 score -22.0 avg score -35.5 time_steps 6906 learning_steps 345\n",
      "episode 35 score -94.0 avg score -37.1 time_steps 7250 learning_steps 362\n",
      "episode 36 score -47.0 avg score -37.4 time_steps 7421 learning_steps 371\n",
      "episode 37 score -41.0 avg score -37.5 time_steps 7673 learning_steps 383\n",
      "episode 38 score -61.5 avg score -38.1 time_steps 7937 learning_steps 396\n",
      "episode 39 score -24.5 avg score -37.8 time_steps 8020 learning_steps 401\n",
      "episode 40 score -3.0 avg score -36.9 time_steps 8092 learning_steps 404\n",
      "episode 41 score -28.5 avg score -36.7 time_steps 8171 learning_steps 408\n",
      "episode 42 score -60.0 avg score -37.3 time_steps 8513 learning_steps 425\n",
      "episode 43 score -23.5 avg score -36.9 time_steps 8675 learning_steps 433\n",
      "episode 44 score -22.0 avg score -36.6 time_steps 8837 learning_steps 441\n",
      "episode 45 score -38.5 avg score -36.7 time_steps 9009 learning_steps 450\n",
      "episode 46 score -44.0 avg score -36.8 time_steps 9261 learning_steps 463\n",
      "episode 47 score -69.0 avg score -37.5 time_steps 9515 learning_steps 475\n",
      "episode 48 score -102.5 avg score -38.8 time_steps 9956 learning_steps 497\n",
      "episode 49 score -4.5 avg score -38.1 time_steps 10028 learning_steps 501\n",
      "episode 50 score -26.5 avg score -37.9 time_steps 10106 learning_steps 505\n",
      "episode 51 score -60.0 avg score -38.3 time_steps 10448 learning_steps 522\n",
      "episode 52 score -25.0 avg score -38.1 time_steps 10532 learning_steps 526\n",
      "episode 53 score -3.0 avg score -37.4 time_steps 10604 learning_steps 530\n",
      "episode 54 score -22.0 avg score -37.1 time_steps 10766 learning_steps 538\n",
      "episode 55 score -41.0 avg score -37.2 time_steps 11018 learning_steps 550\n",
      "episode 56 score -3.0 avg score -36.6 time_steps 11090 learning_steps 554\n",
      "episode 57 score -38.5 avg score -36.6 time_steps 11343 learning_steps 567\n",
      "episode 58 score -44.5 avg score -36.8 time_steps 11515 learning_steps 575\n",
      "episode 59 score 1.5 avg score -36.1 time_steps 11590 learning_steps 579\n",
      "episode 60 score -76.0 avg score -36.8 time_steps 12025 learning_steps 601\n",
      "episode 61 score -3.0 avg score -36.2 time_steps 12097 learning_steps 604\n",
      "episode 62 score -84.5 avg score -37.0 time_steps 12447 learning_steps 622\n",
      "episode 63 score -98.0 avg score -38.0 time_steps 12969 learning_steps 648\n",
      "episode 64 score -28.5 avg score -37.8 time_steps 13048 learning_steps 652\n",
      "episode 65 score -24.0 avg score -37.6 time_steps 13133 learning_steps 656\n",
      "episode 66 score -23.5 avg score -37.4 time_steps 13295 learning_steps 664\n",
      "episode 67 score 0.0 avg score -36.8 time_steps 13367 learning_steps 668\n",
      "episode 68 score -20.5 avg score -36.6 time_steps 13529 learning_steps 676\n",
      "episode 69 score -2.0 avg score -36.1 time_steps 13602 learning_steps 680\n",
      "episode 70 score -22.0 avg score -35.9 time_steps 13764 learning_steps 688\n",
      "episode 71 score -3.0 avg score -35.5 time_steps 13836 learning_steps 691\n",
      "episode 72 score -22.0 avg score -35.3 time_steps 13998 learning_steps 699\n",
      "episode 73 score -23.5 avg score -35.1 time_steps 14082 learning_steps 704\n",
      "episode 74 score -79.5 avg score -35.7 time_steps 14437 learning_steps 721\n",
      "episode 75 score -46.0 avg score -35.8 time_steps 14609 learning_steps 730\n",
      "episode 76 score -48.0 avg score -36.0 time_steps 14779 learning_steps 738\n",
      "episode 77 score -28.0 avg score -35.9 time_steps 14860 learning_steps 743\n",
      "episode 78 score -46.5 avg score -36.0 time_steps 15030 learning_steps 751\n",
      "episode 79 score -99.5 avg score -36.8 time_steps 15474 learning_steps 773\n",
      "episode 80 score -104.0 avg score -37.7 time_steps 15915 learning_steps 795\n",
      "episode 81 score -23.5 avg score -37.5 time_steps 15999 learning_steps 799\n",
      "episode 82 score -6.0 avg score -37.1 time_steps 16071 learning_steps 803\n",
      "episode 83 score -17.5 avg score -36.9 time_steps 16233 learning_steps 811\n",
      "episode 84 score -24.5 avg score -36.7 time_steps 16313 learning_steps 815\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5n/lggt_xf57s78jjmq538wdr6w0000gn/T/ipykernel_2463/2484878506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mlearn_iters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5n/lggt_xf57s78jjmq538wdr6w0000gn/T/ipykernel_2463/826599221.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mactor_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mcritic_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mactor_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mcritic_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MinimumGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_MinimumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m   \u001b[0;34m\"\"\"Returns grad*(x <= y, x > y) with type of grad.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1600\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_MaximumMinimumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless_equal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iannwtf/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MaximumMinimumGrad\u001b[0;34m(op, grad, selector_op)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_MaximumMinimumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselector_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m   \u001b[0;34m\"\"\"Factor out the code for the gradient of Maximum or Minimum.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m   \u001b[0mskip_input_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = ChromeDinoEnv()\n",
    "    #env = gym.make('CartPole-v0')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, alpha=alpha, n_epochs=n_epochs, input_dims=env.observation_space.shape)\n",
    "    n_games = 2000\n",
    "    \n",
    "    figure_file = 'plots/ChromeDino.png'\n",
    "    \n",
    "    best_score = env.reward_range[0]\n",
    "    score_hist = []\n",
    "    \n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    \n",
    "    for i in range(n_games):\n",
    "        #env.reset()\n",
    "        observation = env.reset()#[0]\n",
    "        #print(f'speed: {env.game_speed}')\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "            #print(f'observation:{observation}')\n",
    "        score_hist.append(score)\n",
    "        avg_score = np.mean(score_hist[-100:])\n",
    "            \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "        \n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f'% avg_score, 'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    x = [i+1 for i in range(len(score_hist))]\n",
    "    #plot_learning_curve(x, score_hist, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60673689",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChromeDinoEnv(render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "939e7072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading models ...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = ChromeDinoEnv(render_mode='human')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, alpha=alpha, n_epochs=n_epochs, input_dims=env.observation_space.shape)\n",
    "    agent.load_models() \n",
    "    \n",
    "    figure_file = 'plots/ChromeDino.png'\n",
    "    \n",
    "    best_score = env.reward_range[0]\n",
    "    score_hist = []\n",
    "    \n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    \n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        #agent.store_transition(observation, action, prob, val, reward, done)\n",
    "        observation = observation_\n",
    "        score_hist.append(score)\n",
    "        avg_score = np.mean(score_hist[-100:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a08810",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "while x < 5:\n",
    "    x = random.randint(0,6)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848c7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8711d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbcb916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbbad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544db63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155f062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff27f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
