{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b48db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e5382a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def generate_batches(self):\n",
    "        # get a sample from the memory\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "        \n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "    \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        # store a memory\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8411994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        # Dense Network with 2 hidden layers of 256 neurons\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu')\n",
    "        self.fc3 = Dense(n_actions, activation='softmax')\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0b9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self, fc1_dims=256, fc2_dims=256):\n",
    "        # Dense Network with 2 hidden layers of 256 neurons\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = Dense(fc1_dims, activation='relu')\n",
    "        self.fc2 = Dense(fc2_dims, activation='relu')\n",
    "        self.fc3 = Dense(1, activation=None)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ea895f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95, policy_clip=0.2, batch_size=64, n_epochs=10, chkpt_dir='models/ppo/'):\n",
    "        # future reward discount\n",
    "        self.gamma = gamma\n",
    "        # clipping value\n",
    "        self.policy_clip = policy_clip\n",
    "        # number of epochs\n",
    "        self.n_epochs = n_epochs\n",
    "        # generalized advantage estimator\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.chkpt_dir = chkpt_dir\n",
    "        \n",
    "        # create actor and critic networks\n",
    "        self.actor = ActorNetwork(n_actions)\n",
    "        self.critic = CriticNetwork()\n",
    "        self.actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=alpha))\n",
    "        # create a memory \n",
    "        self.memory = PPOMemory(batch_size)\n",
    "        \n",
    "    def store_transition(self, state, actions, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, actions, probs, vals, reward, done)\n",
    "        \n",
    "    def save_model(self):\n",
    "        self.actor.save(self.chkpt_dir + 'actor')\n",
    "        self.critic.save(self.chkpt_dir + 'critic')\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.actor = keras.models.load_model(self.chkpt_dir + 'actor')\n",
    "        self.critic = keras.models.load_model(self.chkpt_dir + 'critic')\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        # get the next action to perform\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        \n",
    "        # we get the next action as a sample from the distribution\n",
    "        probs = self.actor(state)\n",
    "        dist = tfp.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        # get the log probabilites\n",
    "        log_prob = dist.log_prob(action)\n",
    "        # get the value of the action according to the critic network\n",
    "        value = self.critic(state)\n",
    "        \n",
    "        action = action.numpy()[0]\n",
    "        value = value.numpy()[0]\n",
    "        log_prob = log_prob.numpy()[0]\n",
    "        \n",
    "        return action, log_prob, value\n",
    "    \n",
    "    def learn(self):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            # sample from memory\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, dones_arr, batches = self.memory.generate_batches()\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "            \n",
    "            # compute the advantages\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*(1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            # \n",
    "            for batch in batches:\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    # convert ppo memory samples to tensors\n",
    "                    states = tf.convert_to_tensor(state_arr[batch])\n",
    "                    old_probs = tf.convert_to_tensor(old_prob_arr[batch])\n",
    "                    actions = tf.convert_to_tensor(action_arr[batch])\n",
    "                    \n",
    "                    probs = self.actor(states)\n",
    "                    dist = tfp.distributions.Categorical(probs)\n",
    "                    new_probs = dist.log_prob(actions)\n",
    "                    \n",
    "                \n",
    "                    critic_value = self.critic(states)\n",
    "                    critic_value = tf.squeeze(critic_value, 1)\n",
    "                    \n",
    "                    prob_ratio = tf.math.exp(new_probs - old_probs)\n",
    "                    \n",
    "                    # clip the distribution\n",
    "                    weighted_probs = advantage[batch] * prob_ratio\n",
    "                    clipped_probs = tf.clip_by_value(prob_ratio, 1-self.policy_clip, 1+self.policy_clip)\n",
    "                    weighted_clipped_probs = clipped_probs * advantage[batch]\n",
    "                    \n",
    "                    # compute clipped actor loss\n",
    "                    actor_loss = -tf.math.minimum(weighted_probs, weighted_clipped_probs)\n",
    "                    actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "                    \n",
    "                    # compute critic loss\n",
    "                    returns = advantage[batch] + values[batch]\n",
    "                    critic_loss = keras.losses.MSE(critic_value, returns)\n",
    "                \n",
    "                # apply gradients to actor and critic\n",
    "                actor_params = self.actor.trainable_variables\n",
    "                critic_params = self.critic.trainable_variables\n",
    "                actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "                critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "                self.actor.optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "                self.critic.optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "                \n",
    "        self.memory.clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
